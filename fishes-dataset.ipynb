{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom tqdm.notebook import tqdm\nimport sys\nimport time\nimport copy\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\nimport torchvision\nimport torchvision.transforms as transforms\nfrom torchvision import models\nfrom torch.utils.data import Dataset, random_split, DataLoader\nimport torchvision.transforms as transforms\nloader=transforms.ToTensor()\n\nfrom PIL import Image","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-01-08T13:13:50.499481Z","iopub.execute_input":"2022-01-08T13:13:50.499791Z","iopub.status.idle":"2022-01-08T13:13:52.159695Z","shell.execute_reply.started":"2022-01-08T13:13:50.499706Z","shell.execute_reply":"2022-01-08T13:13:52.158978Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import os\nimport shutil\ndata_dir=r'../input/a-large-scale-fish-dataset/Fish_Dataset/Fish_Dataset'\nos.listdir(data_dir)","metadata":{"execution":{"iopub.status.busy":"2022-01-08T13:13:57.162848Z","iopub.execute_input":"2022-01-08T13:13:57.163117Z","iopub.status.idle":"2022-01-08T13:13:57.185530Z","shell.execute_reply.started":"2022-01-08T13:13:57.163088Z","shell.execute_reply":"2022-01-08T13:13:57.184847Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"root=r'../input/a-large-scale-fish-dataset/Fish_Dataset/Fish_Dataset'\n\nIMG_EXTENSIONS = ['.jpg', '.jpeg', '.png', '.ppm', '.bmp', '.pgm', '.tif', '.tiff', 'webp']\ndef has_file_allowed_extension(filename, extensions):\n    \"\"\"Check if the file is supported scalable type\n\n    Args:\n        filename (string): file path \n                 Extensions: Scalable type list, an acceptable image file type\n\n    Returns:\n        bool: True if the filename ends with one of given extensions\n    \"\"\"\n    filename_lower = filename.lower()\n    return any(filename_lower.endswith(ext) for ext in  extensions) # Return to True or False list\n\n\ndef make_dataset(dir, class_to_idx, extensions):\n    \"\"\"\n                Return, such as [(image path, category index value corresponding to), (), ...]\n    \"\"\"\n    images = []\n    dir = os.path.expanduser(dir)\n    for target in class_to_idx.keys():\n        d=os.path.join(dir,target,target)\n        print(d)\n        if not os.path.isdir(d):\n            continue\n        for root, _, fnames in  sorted(os.walk (d)):\n            for fname in fnames:\n                if  has_file_allowed_extension(fname, extensions): #Viewing if the file is supported, it is continued\n                    path = os.path.join(root, fname)\n                    item = (path, class_to_idx[target])\n                    images.append(item)\n    return images\n                \n    \nclass CustomDataFolder(Dataset):\n    \n    def __init__(self, root, loader, extensions, transform=None, target_transform=None):\n        \n        def find_classes( dir):\n            classes=[]\n            for class_ in os.listdir(dir):\n                if os.path.isdir(root+'/'+class_):\n                    classes.append(class_)\n            class_to_idx = {classes[i]: i for i in  range(len(classes))}\n            return classes, class_to_idx\n        \n        classes, class_to_idx = find_classes (root) # get class name and class index, such as ['cat', 'dog']with{'cat': 0, 'dog': 1}\n                 #       [(image path, the category value of the image), (), ...], that is, tag each image\n        samples = make_dataset(root, class_to_idx, extensions) \n        if len(samples) == 0:\n            raise(RuntimeError(\"Found 0 files in subfolders of: \" + root + \"\\n\"\n                               \"Supported extensions are: \" + \",\".join(extensions)))\n\n        self.root = root\n        self.loader = loader\n        self.extensions = extensions\n\n        self.classes = classes\n        self.class_to_idx = class_to_idx\n        self.samples = samples\n        self.targets = [s[1] for s in  samples] # Lists consisting of all images\n\n        self.transform = transform\n        self.target_transform = target_transform\n    \n    def __getitem__(self, index):\n        \"\"\"\n        Args:\n            index (int): Index\n\n        Returns:\n            tuple: (sample, target) where target is class_index of the target class.\n        \"\"\"\n        path, target = self.samples[index]\n        image = Image.open(path)\n        sample =  self.loader (image) # loading pictures\n        if self.transform is not None:\n            sample = self.transform(sample)\n        if self.target_transform is not None:\n            target = self.target_transform(target)\n\n        return sample, target\n    \n\n    \n    def __len__(self):\n        return len(self.samples)\n\n    def __repr__(self):\n        fmt_str = 'Dataset ' + self.__class__.__name__ + '\\n'\n        fmt_str += '    Number of datapoints: {}\\n'.format(self.__len__())\n        fmt_str += '    Root Location: {}\\n'.format(self.root)\n        tmp = '    Transforms (if any): '\n        fmt_str += '{0}{1}\\n'.format(tmp, self.transform.__repr__().replace('\\n', '\\n' + ' ' * len(tmp)))\n        tmp = '    Target Transforms (if any): '\n        fmt_str += '{0}{1}'.format(tmp, self.target_transform.__repr__().replace('\\n', '\\n' + ' ' * len(tmp)))\n        return fmt_str","metadata":{"execution":{"iopub.status.busy":"2022-01-08T13:14:02.609041Z","iopub.execute_input":"2022-01-08T13:14:02.609300Z","iopub.status.idle":"2022-01-08T13:14:02.627968Z","shell.execute_reply.started":"2022-01-08T13:14:02.609273Z","shell.execute_reply":"2022-01-08T13:14:02.627291Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"class CustomImageFolder(CustomDataFolder):\n    \"\"\"A generic data loader where the images are arranged in this way: ::\n\n        root/dog/xxx.png\n        root/dog/xxy.png\n        root/dog/xxz.png\n\n        root/cat/123.png\n        root/cat/nsdf3.png\n        root/cat/asd932_.png\"\"\"\n    def __init__(self, root, transform=None, target_transform=None,\n                 loader=loader):\n        super(CustomImageFolder, self).__init__(root, loader, IMG_EXTENSIONS,\n                                          transform=transform,\n                                          target_transform=target_transform)\n        self.imgs = self.samples","metadata":{"execution":{"iopub.status.busy":"2022-01-08T13:14:08.153906Z","iopub.execute_input":"2022-01-08T13:14:08.154482Z","iopub.status.idle":"2022-01-08T13:14:08.159559Z","shell.execute_reply.started":"2022-01-08T13:14:08.154444Z","shell.execute_reply":"2022-01-08T13:14:08.158864Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"\nimport torchvision.transforms as tt\ntransform = tt.Compose([\n                         tt.Scale((400,400)), \n                         # tt.RandomRotate\n                         # tt.RandomResizedCrop(256, scale=(0.5,0.9), ratio=(1, 1)), \n                         # tt.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n                         ])\n\ndataset=CustomImageFolder(root,transform=transform)","metadata":{"execution":{"iopub.status.busy":"2022-01-08T13:14:36.862471Z","iopub.execute_input":"2022-01-08T13:14:36.862757Z","iopub.status.idle":"2022-01-08T13:14:39.326652Z","shell.execute_reply.started":"2022-01-08T13:14:36.862724Z","shell.execute_reply":"2022-01-08T13:14:39.325952Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"img,label=dataset[5000]\nprint(img.shape,label)","metadata":{"execution":{"iopub.status.busy":"2022-01-08T13:14:44.041739Z","iopub.execute_input":"2022-01-08T13:14:44.042007Z","iopub.status.idle":"2022-01-08T13:14:44.144708Z","shell.execute_reply.started":"2022-01-08T13:14:44.041978Z","shell.execute_reply":"2022-01-08T13:14:44.143997Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"dataset.classes","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def show_example(img, label):\n    print('Label: ', dataset.classes[label], \"(\"+str(label)+\")\")\n    plt.imshow(img.permute(1, 2, 0))\nshow_example(*dataset[10])","metadata":{"execution":{"iopub.status.busy":"2022-01-08T13:08:17.565797Z","iopub.execute_input":"2022-01-08T13:08:17.566597Z","iopub.status.idle":"2022-01-08T13:08:17.882879Z","shell.execute_reply.started":"2022-01-08T13:08:17.566549Z","shell.execute_reply":"2022-01-08T13:08:17.882198Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"show_example(*dataset[5000])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"show_example(*dataset[7000])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Divide Data into train and test","metadata":{}},{"cell_type":"code","source":"random_seed = 42\ntorch.manual_seed(random_seed);\n","metadata":{"execution":{"iopub.status.busy":"2022-01-08T13:14:54.849048Z","iopub.execute_input":"2022-01-08T13:14:54.849917Z","iopub.status.idle":"2022-01-08T13:14:54.856261Z","shell.execute_reply.started":"2022-01-08T13:14:54.849868Z","shell.execute_reply":"2022-01-08T13:14:54.855510Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"val_size=900\ntrain_size=len(dataset)-val_size\n\ntrain_ds,valid_ds= random_split(dataset, [train_size, val_size])\nprint(len(train_ds),len(valid_ds))\n","metadata":{"execution":{"iopub.status.busy":"2022-01-08T13:14:59.838563Z","iopub.execute_input":"2022-01-08T13:14:59.838820Z","iopub.status.idle":"2022-01-08T13:14:59.853074Z","shell.execute_reply.started":"2022-01-08T13:14:59.838790Z","shell.execute_reply":"2022-01-08T13:14:59.851800Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"`Load the data into batches`","metadata":{}},{"cell_type":"code","source":"from torch.utils.data.dataloader import DataLoader\n\nbatch_size=16","metadata":{"execution":{"iopub.status.busy":"2022-01-08T13:15:10.364033Z","iopub.execute_input":"2022-01-08T13:15:10.364784Z","iopub.status.idle":"2022-01-08T13:15:10.370376Z","shell.execute_reply.started":"2022-01-08T13:15:10.364747Z","shell.execute_reply":"2022-01-08T13:15:10.369652Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"train_dl = DataLoader(train_ds, batch_size, shuffle=True, num_workers=2, pin_memory=True)\nval_dl = DataLoader(valid_ds, batch_size, num_workers=2, pin_memory=True)","metadata":{"execution":{"iopub.status.busy":"2022-01-08T13:15:16.046445Z","iopub.execute_input":"2022-01-08T13:15:16.046708Z","iopub.status.idle":"2022-01-08T13:15:16.051372Z","shell.execute_reply.started":"2022-01-08T13:15:16.046678Z","shell.execute_reply":"2022-01-08T13:15:16.050590Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"for img,label in train_dl:\n    print(img.shape)\n    print(label)\n    break","metadata":{"execution":{"iopub.status.busy":"2022-01-08T13:15:20.861299Z","iopub.execute_input":"2022-01-08T13:15:20.861840Z","iopub.status.idle":"2022-01-08T13:15:25.230571Z","shell.execute_reply.started":"2022-01-08T13:15:20.861800Z","shell.execute_reply":"2022-01-08T13:15:25.229760Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"## Show Multiple Images in Grid","metadata":{}},{"cell_type":"code","source":"from torchvision.utils import make_grid\n\ndef show_batch(dl):\n    for images, labels in dl:\n        fig, ax = plt.subplots(figsize=(12, 6))\n        ax.set_xticks([]); ax.set_yticks([])\n        ax.imshow(make_grid(images, nrow=4).permute(1, 2, 0))\n        break\nshow_batch(train_dl)","metadata":{"execution":{"iopub.status.busy":"2022-01-08T13:15:53.601040Z","iopub.execute_input":"2022-01-08T13:15:53.601628Z","iopub.status.idle":"2022-01-08T13:15:55.815301Z","shell.execute_reply.started":"2022-01-08T13:15:53.601588Z","shell.execute_reply":"2022-01-08T13:15:55.814385Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"## GPU settings","metadata":{}},{"cell_type":"code","source":"def get_default_device():\n    if torch.cuda.is_available():\n        return torch.device('cuda')\n    else:\n        return torch.device('cpu')\ndef to_device(data, device):\n    \"\"\"Move tensor(s) to chosen device\"\"\"\n    if isinstance(data, (list,tuple)):\n        return [to_device(x, device) for x in data]\n    return data.to(device, non_blocking=True)\n\nclass DeviceDataLoader():\n    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n    def __init__(self, dl, device):\n        self.dl = dl\n        self.device = device\n        \n    def __iter__(self):\n        \"\"\"Yield a batch of data after moving it to device\"\"\"\n        for b in self.dl: \n            yield to_device(b, self.device)\n\n    def __len__(self):\n        \"\"\"Number of batches\"\"\"\n        return len(self.dl)","metadata":{"execution":{"iopub.status.busy":"2022-01-08T13:15:38.273890Z","iopub.execute_input":"2022-01-08T13:15:38.274414Z","iopub.status.idle":"2022-01-08T13:15:38.289100Z","shell.execute_reply.started":"2022-01-08T13:15:38.274377Z","shell.execute_reply":"2022-01-08T13:15:38.288149Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"device = get_default_device()\ndevice","metadata":{"execution":{"iopub.status.busy":"2022-01-08T13:15:47.761376Z","iopub.execute_input":"2022-01-08T13:15:47.761646Z","iopub.status.idle":"2022-01-08T13:15:47.767022Z","shell.execute_reply.started":"2022-01-08T13:15:47.761614Z","shell.execute_reply":"2022-01-08T13:15:47.766320Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"## Load data to GPU","metadata":{}},{"cell_type":"code","source":"train_dl = DeviceDataLoader(train_dl, device)\nvalid_dl = DeviceDataLoader(val_dl, device)","metadata":{"execution":{"iopub.status.busy":"2022-01-08T13:16:07.553848Z","iopub.execute_input":"2022-01-08T13:16:07.554282Z","iopub.status.idle":"2022-01-08T13:16:07.560184Z","shell.execute_reply.started":"2022-01-08T13:16:07.554238Z","shell.execute_reply":"2022-01-08T13:16:07.559401Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"## Plotting Functions","metadata":{}},{"cell_type":"code","source":"def plot_accuracies(history):\n    accuracies = [x['val_acc'] for x in history]\n    plt.plot(accuracies, '-x')\n    plt.xlabel('epoch')\n    plt.ylabel('accuracy')\n    plt.title('Accuracy vs. No. of epochs');\n    \ndef plot_losses(history):\n    train_losses = [x.get('train_loss') for x in history]\n    val_losses = [x['val_loss'] for x in history]\n    plt.plot(train_losses, '-bx')\n    plt.plot(val_losses, '-rx')\n    plt.xlabel('epoch')\n    plt.ylabel('loss')\n    plt.legend(['Training', 'Validation'])\n    plt.title('Loss vs. No. of epochs');","metadata":{"execution":{"iopub.status.busy":"2022-01-08T13:16:11.267127Z","iopub.execute_input":"2022-01-08T13:16:11.267600Z","iopub.status.idle":"2022-01-08T13:16:11.273619Z","shell.execute_reply.started":"2022-01-08T13:16:11.267564Z","shell.execute_reply":"2022-01-08T13:16:11.272960Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"## Base Class Module","metadata":{}},{"cell_type":"code","source":"def accuracy(outputs, labels):\n    _, preds = torch.max(outputs, dim=1)\n    return torch.tensor(torch.sum(preds == labels).item() / len(preds))\n\nclass ImageClassificationBase(nn.Module):\n    def training_step(self, batch):\n        images, labels = batch \n        out = self(images)                  # Generate predictions\n        loss = F.cross_entropy(out, labels) # Calculate loss\n        return loss\n    \n    def validation_step(self, batch):\n        images, labels = batch \n        out = self(images)                    # Generate predictions\n        loss = F.cross_entropy(out, labels)   # Calculate loss\n        acc = accuracy(out, labels)           # Calculate accuracy\n        return {'val_loss': loss.detach(), 'val_acc': acc}\n        \n    def validation_epoch_end(self, outputs):\n        batch_losses = [x['val_loss'] for x in outputs]\n        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n        batch_accs = [x['val_acc'] for x in outputs]\n        epoch_acc = torch.stack(batch_accs).mean()      # Combine accuracies\n        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n    \n    def epoch_end(self, epoch, result):\n        print(\"Epoch [{}], train_loss: {:.4f}, val_loss: {:.4f}, val_acc: {:.4f}\".format(\n            epoch, result['train_loss'], result['val_loss'], result['val_acc']))","metadata":{"execution":{"iopub.status.busy":"2022-01-08T13:16:14.884572Z","iopub.execute_input":"2022-01-08T13:16:14.885151Z","iopub.status.idle":"2022-01-08T13:16:14.895120Z","shell.execute_reply.started":"2022-01-08T13:16:14.885111Z","shell.execute_reply":"2022-01-08T13:16:14.894286Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"## Resnet Architecture","metadata":{}},{"cell_type":"markdown","source":"`Resnet9 Architecture`","metadata":{}},{"cell_type":"markdown","source":"![](http://raw.githubusercontent.com/lambdal/cifar10-fast/master/net.svg)","metadata":{}},{"cell_type":"markdown","source":"## Single Block\n![](http://miro.medium.com/max/1140/1*D0F3UitQ2l5Q0Ak-tjEdJg.png)","metadata":{}},{"cell_type":"code","source":"class SimpleResidualBlock(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(in_channels=3, out_channels=3, kernel_size=3, stride=1, padding=1)\n        self.relu1 = nn.ReLU()\n        self.conv2 = nn.Conv2d(in_channels=3, out_channels=3, kernel_size=3, stride=1, padding=1)\n        self.relu2 = nn.ReLU()\n        \n    def forward(self, x):\n        out = self.conv1(x)\n        out = self.relu1(out)\n        out = self.conv2(out)\n        return self.relu2(out) + x # ReLU can be applied before or after adding the input","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Test the residual block","metadata":{}},{"cell_type":"code","source":"simple_resnet = to_device(SimpleResidualBlock(), device)\n\nfor images, labels in train_dl:\n    out = simple_resnet(images)\n    print(out.shape)\n    break\n    \ndel simple_resnet, images, labels\ntorch.cuda.empty_cache()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Main Resnet Architecture","metadata":{}},{"cell_type":"code","source":"def conv_block(in_channels,out_channels,pool=False):\n    layers=[nn.Conv2d(in_channels,out_channels,kernel_size=3,padding=1),\n           nn.BatchNorm2d(out_channels),\n           nn.ReLU(inplace=True)]\n    if pool: layers.append(nn.MaxPool2d(2))\n    return nn.Sequential(*layers)\n\nclass ResNet9(ImageClassificationBase):\n    def __init__(self,in_channels,num_classes):\n        super().__init__()\n        self.conv1=conv_block(in_channels,64)\n        self.conv2=conv_block(64,128,pool=True)\n        self.res1=nn.Sequential(conv_block(128, 128), conv_block(128, 128))\n        self.conv3=conv_block(128, 256, pool=True)\n        self.conv4=conv_block(256, 512, pool=True)\n        self.res2=nn.Sequential(conv_block(512, 512), conv_block(512, 512))\n        self.classifier = nn.Sequential(nn.MaxPool2d(4), \n                                        nn.Flatten(), \n                                        nn.Dropout(0.2),\n                                        nn.Linear(73728, num_classes))\n    def forward(self, xb):\n        out = self.conv1(xb)\n        out = self.conv2(out)\n        out = self.res1(out) + out\n        out = self.conv3(out)\n        out = self.conv4(out)\n        out = self.res2(out) + out\n        out = self.classifier(out)\n        return out    \n    \nmodel = to_device(ResNet9(3, 9), device)\nmodel\n# After res2 torch.Size([64, 512, 50, 50])\n# After maxpool torch.Size([64, 512, 12, 12])\n# torch.Size([64, 73728])\n# torch.Size([64, 73728])","metadata":{"execution":{"iopub.status.busy":"2022-01-08T13:16:22.876817Z","iopub.execute_input":"2022-01-08T13:16:22.877468Z","iopub.status.idle":"2022-01-08T13:16:22.969858Z","shell.execute_reply.started":"2022-01-08T13:16:22.877424Z","shell.execute_reply":"2022-01-08T13:16:22.969110Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"torch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2022-01-08T13:17:22.342351Z","iopub.execute_input":"2022-01-08T13:17:22.342638Z","iopub.status.idle":"2022-01-08T13:17:22.387547Z","shell.execute_reply.started":"2022-01-08T13:17:22.342605Z","shell.execute_reply":"2022-01-08T13:17:22.386782Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"## Paramters\n","metadata":{}},{"cell_type":"code","source":"batch_size = 32\nn_iters = 30\nepochs  = 10#int( n_iters / (len(train_dl) / batch_size))\ninput_dim = 3*400*400\noutput_dim = 9\nlr_rate  = 0.001","metadata":{"execution":{"iopub.status.busy":"2022-01-08T13:09:24.864122Z","iopub.execute_input":"2022-01-08T13:09:24.864400Z","iopub.status.idle":"2022-01-08T13:09:24.868843Z","shell.execute_reply.started":"2022-01-08T13:09:24.864366Z","shell.execute_reply":"2022-01-08T13:09:24.867883Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"## Training the Model","metadata":{}},{"cell_type":"code","source":"@torch.no_grad()\ndef evaluate(model, val_loader):\n    model.eval()\n    outputs = [model.validation_step(batch) for batch in val_loader]\n    return model.validation_epoch_end(outputs)\n\ndef get_lr(optimizer):\n    for param_group in optimizer.param_groups:\n        return param_group['lr']\n\n\ndef fit_one_cycle(epochs, max_lr, model, train_loader, val_loader, \n                  weight_decay=0, grad_clip=None, opt_func=torch.optim.SGD):\n    torch.cuda.empty_cache()\n    history = []\n    \n    # Set up cutom optimizer with weight decay\n    optimizer = opt_func(model.parameters(), max_lr, weight_decay=weight_decay)\n    # Set up one-cycle learning rate scheduler\n    sched = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr, epochs=epochs, \n                                                steps_per_epoch=len(train_loader))\n    \n    for epoch in range(epochs):\n        # Training Phase \n        model.train()\n        train_losses = []\n        lrs = []\n        for batch in train_loader:\n            loss = model.training_step(batch)\n            train_losses.append(loss)\n            loss.backward()\n            \n            # Gradient clipping\n            if grad_clip: \n                nn.utils.clip_grad_value_(model.parameters(), grad_clip)\n            \n            optimizer.step()\n            optimizer.zero_grad()\n            \n            # Record & update learning rate\n            lrs.append(get_lr(optimizer))\n            sched.step()\n        \n        # Validation phase\n        result = evaluate(model, val_loader)\n        result['train_loss'] = torch.stack(train_losses).mean().item()\n        result['lrs'] = lrs\n        model.epoch_end(epoch, result)\n        history.append(result)\n    return history","metadata":{"execution":{"iopub.status.busy":"2022-01-08T13:16:34.421451Z","iopub.execute_input":"2022-01-08T13:16:34.421738Z","iopub.status.idle":"2022-01-08T13:16:34.432596Z","shell.execute_reply.started":"2022-01-08T13:16:34.421691Z","shell.execute_reply":"2022-01-08T13:16:34.431830Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"for img,label in valid_dl:\n    print(img.shape)\n    print(label)\n    break","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = [evaluate(model, valid_dl)]\nhistory","metadata":{"execution":{"iopub.status.busy":"2022-01-08T13:16:39.906950Z","iopub.execute_input":"2022-01-08T13:16:39.907510Z","iopub.status.idle":"2022-01-08T13:17:04.964751Z","shell.execute_reply.started":"2022-01-08T13:16:39.907473Z","shell.execute_reply":"2022-01-08T13:17:04.963987Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"epochs = 8\nmax_lr = 0.01\ngrad_clip = 0.1\nweight_decay = 1e-4\nopt_func = torch.optim.Adam","metadata":{"execution":{"iopub.status.busy":"2022-01-08T13:17:50.288266Z","iopub.execute_input":"2022-01-08T13:17:50.288542Z","iopub.status.idle":"2022-01-08T13:17:50.292870Z","shell.execute_reply.started":"2022-01-08T13:17:50.288505Z","shell.execute_reply":"2022-01-08T13:17:50.292179Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"%%time\nhistory += fit_one_cycle(epochs, max_lr, model, train_dl, valid_dl, \n                             grad_clip=grad_clip, \n                             weight_decay=weight_decay, \n                             opt_func=opt_func)","metadata":{"execution":{"iopub.status.busy":"2022-01-08T13:17:59.010719Z","iopub.execute_input":"2022-01-08T13:17:59.011612Z","iopub.status.idle":"2022-01-08T14:05:17.574487Z","shell.execute_reply.started":"2022-01-08T13:17:59.011571Z","shell.execute_reply":"2022-01-08T14:05:17.573660Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"plot_accuracies(history)","metadata":{"execution":{"iopub.status.busy":"2022-01-08T14:08:31.600239Z","iopub.execute_input":"2022-01-08T14:08:31.600556Z","iopub.status.idle":"2022-01-08T14:08:31.829101Z","shell.execute_reply.started":"2022-01-08T14:08:31.600521Z","shell.execute_reply":"2022-01-08T14:08:31.828446Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"plot_losses(history)","metadata":{"execution":{"iopub.status.busy":"2022-01-08T14:08:48.642817Z","iopub.execute_input":"2022-01-08T14:08:48.643415Z","iopub.status.idle":"2022-01-08T14:08:48.867478Z","shell.execute_reply.started":"2022-01-08T14:08:48.643376Z","shell.execute_reply":"2022-01-08T14:08:48.866803Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"markdown","source":"## Test the images","metadata":{}},{"cell_type":"code","source":"from torchvision.datasets import ImageFolder\npath='../input/a-large-scale-fish-dataset/NA_Fish_Dataset'\nimport torchvision.transforms as tt\ntransform = tt.Compose([\n                         tt.Scale((400,400)), \n                         tt.ToTensor()\n                         # tt.RandomRotate\n                         # tt.RandomResizedCrop(256, scale=(0.5,0.9), ratio=(1, 1)), \n                         # tt.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n                         ])\ntest_data=ImageFolder(path,transform=transform)\ntest_data.classes","metadata":{"execution":{"iopub.status.busy":"2022-01-08T14:07:54.851348Z","iopub.execute_input":"2022-01-08T14:07:54.851627Z","iopub.status.idle":"2022-01-08T14:07:54.884224Z","shell.execute_reply.started":"2022-01-08T14:07:54.851598Z","shell.execute_reply":"2022-01-08T14:07:54.883571Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"for img,label in test_data:\n    print(img.shape)\n    print(label)\n    break","metadata":{"execution":{"iopub.status.busy":"2022-01-08T14:07:58.975673Z","iopub.execute_input":"2022-01-08T14:07:58.976545Z","iopub.status.idle":"2022-01-08T14:07:59.005323Z","shell.execute_reply.started":"2022-01-08T14:07:58.976499Z","shell.execute_reply":"2022-01-08T14:07:59.004618Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"def predict_image(img,model):\n    # Convert to a batch of 1\n    xb = to_device(img.unsqueeze(0), device)\n    # Get predictions from model\n    yb = model(xb)\n    # Pick index with highest probability\n    _, preds  = torch.max(yb, dim=1)\n    # Retrieve the class label\n    return test_data.classes[preds[0].item()]\n","metadata":{"execution":{"iopub.status.busy":"2022-01-08T14:10:33.324832Z","iopub.execute_input":"2022-01-08T14:10:33.325116Z","iopub.status.idle":"2022-01-08T14:10:33.329985Z","shell.execute_reply.started":"2022-01-08T14:10:33.325085Z","shell.execute_reply":"2022-01-08T14:10:33.329246Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"img,label=test_data[34]\nplt.imshow(img.permute(1, 2, 0).clamp(0, 1))\nprint('Label:', test_data.classes[label], ', Predicted:', predict_image(img, model))","metadata":{"execution":{"iopub.status.busy":"2022-01-08T14:14:01.735876Z","iopub.execute_input":"2022-01-08T14:14:01.736579Z","iopub.status.idle":"2022-01-08T14:14:02.050957Z","shell.execute_reply.started":"2022-01-08T14:14:01.736545Z","shell.execute_reply":"2022-01-08T14:14:02.048173Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"markdown","source":"## Save the Parameters","metadata":{}},{"cell_type":"code","source":"torch.save(model.state_dict(), 'fishes_large_scale-resnet9.pth')","metadata":{"execution":{"iopub.status.busy":"2022-01-08T14:14:09.351747Z","iopub.execute_input":"2022-01-08T14:14:09.352020Z","iopub.status.idle":"2022-01-08T14:14:09.408455Z","shell.execute_reply.started":"2022-01-08T14:14:09.351990Z","shell.execute_reply":"2022-01-08T14:14:09.407725Z"},"trusted":true},"execution_count":45,"outputs":[]}]}